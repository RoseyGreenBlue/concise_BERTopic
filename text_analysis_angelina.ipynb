{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"private_outputs":true,"gpuType":"T4","authorship_tag":"ABX9TyP85AT0mEnvkS9TcKksmCj1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!pip install bertopic"],"metadata":{"id":"vlmAju6Pg8sP"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HQSjjv0IfBR8"},"outputs":[],"source":["import pandas as pd\n","from google.colab import drive\n","import umap\n","import sqlalchemy\n","from hdbscan import HDBSCAN\n","from sentence_transformers import SentenceTransformer\n","from sklearn.feature_extraction.text import CountVectorizer\n","from scipy.sparse import csr_matrix\n","from bertopic.representation import KeyBERTInspired\n","from bertopic.vectorizers import ClassTfidfTransformer\n","\n","from scipy.spatial import KDTree\n","\n","import nltk\n","from sentence_transformers import SentenceTransformer\n","import random\n","\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","import string\n","import collections\n","from typing import List\n","import re\n","\n","from sklearn.metrics.pairwise import cosine_similarity\n","import sys\n","import os\n","import json\n","import numpy as np\n","from pprint import pprint\n","import torch\n","\n","from random import shuffle"]},{"cell_type":"code","source":["drive.mount('/content/drive')"],"metadata":{"id":"FXdngwP4lyry"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class CustomClusterer():\n","  def __init__(self, num_reduced_dimensions: int, num_top_words: int, bm25_weighting: bool = False, reduce_frequent_words: bool = False):\n","    self.embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n","    self.umap_model = umap.UMAP(n_neighbors=15, n_components=num_reduced_dimensions, min_dist=0.0, metric='cosine')\n","    self.hdbscan_model = HDBSCAN(min_cluster_size=15, metric='euclidean', cluster_selection_method='eom', prediction_data=True)\n","    self.vectorizer_model = CountVectorizer(stop_words=\"english\")\n","    self.ctfidf_model = ClassTfidfTransformer(bm25_weighting=bm25_weighting, reduce_frequent_words=reduce_frequent_words)\n","\n","    self.num_top_words = num_top_words\n","\n","  def preprocess_text(self, documents: np.ndarray) -> List[str]:\n","    \"\"\" Basic preprocessing of text\n","\n","    Steps:\n","        * Replace \\n and \\t with whitespace\n","        * Only keep alpha-numerical characters\n","    \"\"\"\n","    cleaned_documents = [doc.replace(\"\\n\", \" \") for doc in documents]\n","    cleaned_documents = [doc.replace(\"\\t\", \" \") for doc in cleaned_documents]\n","    # assuming language is english\n","    cleaned_documents = [re.sub(r'[^A-Za-z0-9 ]+', '', doc) for doc in cleaned_documents]\n","    cleaned_documents = [doc if doc != \"\" else \"emptydoc\" for doc in cleaned_documents]\n","    return cleaned_documents\n","\n","\n","  def top_n_idx_sparse(self, matrix: csr_matrix, n: int) -> np.ndarray:\n","    \"\"\" Return indices of top n values in each row of a sparse matrix\n","\n","    Retrieved from:\n","        https://stackoverflow.com/questions/49207275/finding-the-top-n-values-in-a-row-of-a-scipy-sparse-matrix\n","\n","    Arguments:\n","        matrix: The sparse matrix from which to get the top n indices per row\n","        n: The number of highest values to extract from each row\n","\n","    Returns:\n","        indices: The top n indices per row\n","    \"\"\"\n","    indices = []\n","    for le, ri in zip(matrix.indptr[:-1], matrix.indptr[1:]):\n","        n_row_pick = min(n, ri - le)\n","        values = matrix.indices[le + np.argpartition(matrix.data[le:ri], -n_row_pick)[-n_row_pick:]]\n","        values = [values[index] if len(values) >= index + 1 else None for index in range(n)]\n","        indices.append(values)\n","    return np.array(indices)\n","\n","  def top_n_values_sparse(self, matrix: csr_matrix, indices: np.ndarray) -> np.ndarray:\n","    \"\"\" Return the top n values for each row in a sparse matrix\n","\n","    Arguments:\n","        matrix: The sparse matrix from which to get the top n indices per row\n","        indices: The top n indices per row\n","\n","    Returns:\n","        top_values: The top n scores per row\n","    \"\"\"\n","    top_values = []\n","    for row, values in enumerate(indices):\n","        scores = np.array([matrix[row, value] if value is not None else 0 for value in values])\n","        top_values.append(scores)\n","    return np.array(top_values)\n","\n","\n","  def create_topic_vectors(self, documents: pd.DataFrame = None, embeddings: np.ndarray = None):\n","    topic_embeddings = []\n","    topics = documents.sort_values(\"topics\").topics.unique()\n","    for topic in topics:\n","        indices = np.where(documents.topics == topic)[0]\n","        indices = [int(index) for index in indices]\n","        topic_embedding = np.mean(embeddings[indices], axis=0)\n","        topic_embeddings.append(topic_embedding)\n","    return dict(zip(topics, np.array(topic_embeddings)))\n","\n","  def guided_topics(self, embeddings: np.ndarray = None, seed_topics = List[List[str]]) -> tuple[List[int], np.array]:\n","    seed_topic_words = [\" \".join(seed_words) for seed_words in seed_topics]\n","    seed_topic_words = self.embedding_model.encode(seed_topic_words)\n","    pass\n","\n","  def fit(self, data: pd.DataFrame, data_key: str, seed_topics:List[List[str]] = None) -> dict:\n","    embeddings = self.embedding_model.encode(data[data_key], show_progress_bar = True)\n","    raw_embeddings = embeddings\n","    self.umap_model.fit(embeddings) # fit to embeddings manifold\n","    embeddings = np.nan_to_num(self.umap_model.transform(X=embeddings))\n","\n","    self.hdbscan_model.fit(embeddings)\n","    data['topics'] = self.hdbscan_model.labels_\n","\n","    # literally all this code does is sort the topics by its frequency, so topic 0 is remapped to the most frequent topic, 1 to the second-most, etc.\n","    # https://github.com/MaartenGr/BERTopic/blob/master/bertopic/_bertopic.py#L3830\n","    size_sorted_topics = pd.DataFrame(collections.Counter(data.topics.values.tolist()).items(), columns=(\"topics\", \"size\")).sort_values(\"size\", ascending=False)\n","    size_sorted_topics = size_sorted_topics[size_sorted_topics.topics != -1] # ignore \"other\" topic for now\n","    size_sorted_topics = {**{-1:-1}, **dict(zip(size_sorted_topics.topics, range(len(size_sorted_topics))))}\n","    data.topics = data.topics.map(size_sorted_topics).fillna(data.topics).astype(int)\n","\n","    # create BIG string for each topic where each string is a concatenation of all the strings that were assigned that topic - returns dict_item\n","    test_data = data.groupby(['topics'], as_index=False).agg({data_key: ' '.join})\n","    test_data = self.preprocess_text(test_data[data_key].values)\n","\n","    self.vectorizer_model.fit(test_data)\n","    X = self.vectorizer_model.transform(test_data)\n","\n","    ctfidf_model = self.ctfidf_model.fit(X, multiplier=None)\n","\n","    c_tf_idf = ctfidf_model.transform(X)\n","    words = self.vectorizer_model.get_feature_names_out()\n","\n","    labels = sorted(list(data.topics.unique()))\n","    labels = [int(label) for label in labels]\n","\n","    # default num words = 50\n","    indices = self.top_n_idx_sparse(c_tf_idf, 50)\n","\n","    scores = self.top_n_values_sparse(c_tf_idf, indices)\n","    sorted_indices = np.argsort(scores, 1)\n","    indices = np.take_along_axis(indices, sorted_indices, axis=1)\n","    scores = np.take_along_axis(scores, sorted_indices, axis=1)\n","\n","    topics = {label: [(words[word_index], score) if word_index is not None and score > 0\n","                else (\"\", 0.00001) for word_index, score in zip(indices[index][::-1], scores[index][::-1])]\n","                for index, label in enumerate(labels)}\n","\n","    topic_vectors = self.create_topic_vectors(data, raw_embeddings)\n","    return {\"data\":data, \"embeddings\":raw_embeddings, \"topic_vectors\":topic_vectors, \"topic_names\":topics}\n","\n","  def inference(self, complaint:str = None, topic_names:dict = None, topic_embeddings:dict = None) -> tuple[list[int], list[float]]:\n","    complaint_embedding = self.embedding_model.encode([complaint])[0]\n","    complaint_embedding = complaint_embedding / np.linalg.norm(complaint_embedding)\n","\n","    topic_num = topic_embeddings.keys()\n","    cosine_similarities = np.zeros((len(topic_num),))\n","\n","    for key in topic_num:\n","      curr_topic_embedding = topic_embeddings[key]\n","      cosine_similarities[key+1] = np.dot(curr_topic_embedding / np.linalg.norm(curr_topic_embedding), complaint_embedding)\n","\n","    indexes = (-1 * cosine_similarities).argsort()\n","    sims = [cosine_similarities[indexes[i]] for i in range(len(indexes))]\n","    top_topics = [topic_names[indexes[i] - 1] for i in range(len(indexes))]\n","\n","    return top_topics, sims, tuple([num - 1 for num in indexes])\n"],"metadata":{"id":"LuAN2l1lf4d1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["engine = CustomClusterer(num_reduced_dimensions=5, num_top_words=20, bm25_weighting=True, reduce_frequent_words=True)"],"metadata":{"id":"qeUCflY9gOeV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["path = \"./drive/MyDrive/data_analysis/.angelina_data.txt\"\n","file_data = []\n","with open(path,\"r\") as data:\n","  file_data = data.readlines()\n","  file_data = [elem.strip() for elem in file_data]\n","  file_data = [elem for elem in file_data if elem != '']\n","\n","sentinel = -1\n","file_data = iter(file_data)\n","\n","angelina = []\n","aayush = []\n","all_msgs = []\n","\n","msg = next(file_data, sentinel)\n","adder = []\n","while (msg != sentinel):\n","  if (msg == 'to AARP Shayla Angelina Riann Puglia'):\n","    adder = aayush\n","  elif (msg == 'from AARP Shayla Angelina Riann Puglia'): #   adder = angelina\n","    adder = angelina\n","  else:\n","    adder.append(msg)\n","    all_msgs.append(msg)\n","  msg = next(file_data, sentinel)"],"metadata":{"id":"eD8PGhR1f5Qw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(len(all_msgs))\n","print(len(aayush))\n","print(len(angelina))"],"metadata":{"id":"kq2PnHINoGLi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["aayush = pd.DataFrame(aayush, columns=[\"data\"])\n","angelina = pd.DataFrame(angelina, columns=[\"data\"])\n","all_msgs = pd.DataFrame(all_msgs, columns=[\"data\"])"],"metadata":{"id":"MN8PTFIe4O-A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_topics(words):\n","  output = engine.fit(words,data_key=\"data\")\n","  tn = output[\"topic_names\"]\n","  all_topics = dict()\n","  for i in tn:\n","    topic = tn[i]\n","    all_topics[i] = [elem[0] for elem in topic]\n","\n","  return (words, all_topics)"],"metadata":{"id":"mfEhAZMK7luW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["aayush = get_topics(aayush)\n"],"metadata":{"id":"NrLSvQKq5Fv3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(len(aayush_topics))\n","pprint(aayush_topics[-1])"],"metadata":{"id":"bUrjzacB8e6M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["angelina = get_topics(angelina)\n"],"metadata":{"id":"vZphHeOo8anv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["current_analysis = all_msg_topics\n","\n","analysis_topics = current_analysis[1]\n","analysis_df = current_analysis[0]\n","\n","N = 590\n","print(f\"{len(analysis_topics)} total topics\")\n","print(f\"current topic: {N}\\n---------------------------\\ntopic description:\")\n","pprint(analysis_topics[N][:10])\n","print(\"---------------------------\\n\")\n","print(\"\\nsample texts\\n---------------------------\")\n","\n","samples = list(analysis_df[analysis_df[\"topics\"] == N][\"data\"])\n","print(len(samples))\n","shuffle(samples)\n","pprint(samples[:50])"],"metadata":{"id":"_w2ISLv09QAx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["all_msg_topics = get_topics(all_msgs)"],"metadata":{"id":"ms0Ph4hd8dDc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["topics_aayush = dict()\n","print(len(tn_aayush))\n","\n","for i in tn_aayush:\n","  topic = tn_aayush[i]\n","  topics_aayush[i] = ([elem[0] for elem in topic])"],"metadata":{"id":"mHgi3Rs06JX6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["angelina_output = engine.fit(angelina, data_key=\"data\")\n","data_angelina = angelina_output[\"data\"]\n","embeddings_angelina = angelina_output[\"data\"]\n","tv_angelina = angelina_output[\"topic_vectors\"]\n","tn_angelina = angelina_output[\"topic_names\"]"],"metadata":{"id":"mX5n5QDA5_Z5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(text_data)"],"metadata":{"id":"5JTOprvQlHCm"},"execution_count":null,"outputs":[]}]}